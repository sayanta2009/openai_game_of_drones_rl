{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "import arlenvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment CreationÂ¶\n",
    "\n",
    "- uncomment the desired environment\n",
    "- init env class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sayanta/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# env_name = \"MexicanWorld-v0\"\n",
    "# env_name = \"ContinuousMexicanWorld-v0\"\n",
    "# env_name = \"MarsLander-v0\"  # gym.make accepts level=1, level=2 or level=3 as kwarg, to control the difficultly\n",
    "# env_name = \"GettingOverIt-v0\"\n",
    "# env_name = \"PlanetWorld-v0\"\n",
    "# env_name = 'GameOfDrones-v0'\n",
    "# env_name = \"FlappyBird-v0\"\n",
    "# env_name = \"DronePathFinding-v0\"\n",
    "# env_name = \"DronePathTracking-v0\"\n",
    "env_name = \"HaxBall-v0\"\n",
    "# unwrapped to get rid of this TimeLimitWrapper, which might reset the environment twice and thus breaks ergodicity\n",
    "env = gym.make(env_name).unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent for environment testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state:  [2.5       1.5       0.        0.        1.5       1.5       0.\n",
      " 0.        4.0999999 1.5      ]\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "-0.001\n",
      "time consumed 2.5126681327819824\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state = env.reset()\n",
    "print(\"Starting state: \",state)\n",
    "\n",
    "counter = 0\n",
    "time_start = time.time()\n",
    "\n",
    "#while True:\n",
    "for i in range(100):\n",
    "  env.render(mode=\"human\")\n",
    "\n",
    "  action = env.action_space.sample()\n",
    "\n",
    "  state, reward, done, _ = env.step(action)\n",
    "\n",
    "  print(reward)\n",
    "\n",
    "  # For very fast running environments ...\n",
    "  time.sleep(0.02)\n",
    "\n",
    "  if done:\n",
    "    print(\"Resetting\")\n",
    "    env.reset()\n",
    "  #end\n",
    "#end\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"time consumed\", time.time() - time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init qtable and policy table\n",
    "\n",
    "- main idea: discretize simple performing a rounding operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  9\n",
      "action sample:  8 8 \n",
      " \n",
      "\n",
      "State size  (4,)\n",
      "State bounds:  [0. 0. 0. 0.] [0. 0. 0. 0.]\n",
      "State dim:  [1. 1. 1. 1.]\n",
      "state sample:  [0. 0. 0. 0.] [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "action_size = env.action_space.n\n",
    "print(\"Action size \", action_size)\n",
    "\n",
    "\n",
    "a_sample = env.action_space.sample()\n",
    "print(\"action sample: \", np.round(a_sample), a_sample, \"\\n \\n\")\n",
    "\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "print(\"State size \", state_size)\n",
    "print(\"State bounds: \", env.observation_space.low, env.observation_space.high)\n",
    "state_dim = env.observation_space.high -env.observation_space.low +1\n",
    "print(\"State dim: \", state_dim )\n",
    "\n",
    "sample = env.observation_space.sample()\n",
    "print(\"state sample: \", np.round(sample), sample)\n",
    "state_dim_real = np.array([20, 20 , 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import perf_counter\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.5      , 1.5      , 0.       , 0.       , 1.5      , 1.5      ,\n",
       "       0.       , 0.       , 4.0999999, 1.5      ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.5       , 1.5       , 0.        , 0.        , 1.50375187,\n",
       "        1.50375187, 0.22511308, 0.22511308, 4.0999999 , 1.5       ]),\n",
       " -0.001,\n",
       " False,\n",
       " {'blue_goal_cnt': 0, 'red_goal_cnt': 0, 'ball_hit_cnt': 0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 500000\n",
    "nA = env.action_space.n\n",
    "Q = defaultdict(lambda: np.zeros(nA, dtype=float))\n",
    "discount_factor = 0.9  \n",
    "epsilon = 0.1\n",
    "env_max_steps = 10000\n",
    "render = False\n",
    "test_episodes = 1000\n",
    "train_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy():\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    Returns:\n",
    "        A function that takes the observation (state) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy_fn(observation):\n",
    "        prob = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        optimal_action = np.argmax(Q[tuple(observation)])\n",
    "        prob[optimal_action] += (1.0 - epsilon)\n",
    "        return prob\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_train():\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    policy_fn = make_epsilon_greedy_policy()\n",
    "    counter = perf_counter()\n",
    "\n",
    "    for i_episode in range(1, train_episodes + 1):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        \n",
    "        if i_episode % 5000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, train_episodes), end=\"\")\n",
    "            print('no of states explored:', len(Q.keys()))\n",
    "            print('Time taken:', (perf_counter() - counter) / 60, 'minutes')\n",
    "            counter = perf_counter()\n",
    "#             render = True\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            render = False\n",
    "\n",
    "        reward_episode = 0.0\n",
    "        for step in range(env_max_steps):\n",
    "            if state in Q:\n",
    "                prob_values = policy_fn(state)\n",
    "                action = np.random.choice(np.arange(nA), p=prob_values)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((next_state, action, reward))\n",
    "            reward_episode += reward\n",
    "\n",
    "            if render:\n",
    "                env.render(mode='rgb_array')\n",
    "\n",
    "            if done:\n",
    "                env.close()\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "        train_rewards.append(reward_episode)\n",
    "        sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurrence of the (state, action) pair in the episode\n",
    "            first_occurrence_idx = next(i for i, x in enumerate(episode)\n",
    "                                        if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurrence\n",
    "            G = sum([x[2] * (discount_factor ** i) for i, x in enumerate(episode[first_occurrence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "\n",
    "    return train_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-21f4966e3a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_control_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-78f0ff041873>\u001b[0m in \u001b[0;36mmc_control_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mreward_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mprob_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "rewards = mc_control_train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
