{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "import arlenvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Environment Creation\n",
    "    \n",
    "* uncomment the desired environment\n",
    "* init env class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"MexicanWorld-v0\"\n",
    "# env_name = \"ContinuousMexicanWorld-v0\"\n",
    "# env_name = \"MarsLander-v0\"  # gym.make accepts level=1, level=2 or level=3 as kwarg, to control the difficultly\n",
    "# env_name = \"GettingOverIt-v0\"\n",
    "# env_name = \"PlanetWorld-v0\"\n",
    "# env_name = 'GameOfDrones-v0'\n",
    "# env_name = \"FlappyBird-v0\"\n",
    "# env_name = \"DronePathFinding-v0\"\n",
    "env_name = \"DronePathTracking-v0\"\n",
    "# env_name = \"HaxBall-v0\"\n",
    "# unwrapped to get rid of this TimeLimitWrapper, which might reset the environment twice and thus breaks ergodicity\n",
    "env = gym.make(env_name).unwrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent for environment testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "-0.06017930052651508\n",
      "-0.12035861114643576\n",
      "-0.1805364966839424\n",
      "-0.24070898360208454\n",
      "-0.30086984043252507\n",
      "-0.3610107990110926\n",
      "-0.42112172347819127\n",
      "-0.4811919766467913\n",
      "-0.5412086156590697\n",
      "-0.6011642344268622\n",
      "-0.6610366134162022\n",
      "-0.7208116102427736\n",
      "-0.7804730806051021\n",
      "-0.8400049825276887\n",
      "-0.8993890771471409\n",
      "-0.9586092760248904\n",
      "-1.0176495646344104\n",
      "-1.0764904973287734\n",
      "-1.1351100321976297\n",
      "-1.1934907382086457\n",
      "-1.2516180454041317\n",
      "-1.309470546489618\n",
      "-1.367035124381341\n",
      "-1.4242944738320589\n",
      "-1.4812305636029488\n",
      "-1.5378190881832663\n",
      "-1.5940486359345927\n",
      "-1.6499005471427632\n",
      "-1.7053396155780434\n",
      "-1.7603601006772276\n",
      "-1.8149596304427253\n",
      "-1.869119267860552\n",
      "-1.9228036382468368\n",
      "-1.9760190549817371\n",
      "-2.028732619880643\n",
      "-2.080945302827958\n",
      "-2.1326328956355525\n",
      "-2.1837618161753993\n",
      "-2.2343472760773353\n",
      "-2.2843822588295293\n",
      "-2.333848016302879\n",
      "-2.382678517359694\n",
      "-2.430927369841689\n",
      "-2.478535548486596\n",
      "-2.5255148679829063\n",
      "-2.5718629515535105\n",
      "-2.617534673714502\n",
      "-2.662520996975122\n",
      "-2.7068528919016197\n",
      "-2.750454933225098\n",
      "-2.793288865697535\n",
      "-2.8353570037477964\n",
      "-2.8767518674668313\n",
      "-2.917294741289548\n",
      "-2.9570350754971306\n",
      "-2.9961416561007055\n",
      "-3.0344241728814856\n",
      "-3.071907772158348\n",
      "-3.1084523420994508\n",
      "-3.144206977920965\n",
      "-3.179244508472854\n",
      "-3.2133068321977594\n",
      "-3.2463897501160304\n",
      "-3.2784885786427926\n",
      "-3.3096953590588964\n",
      "-3.3402104233519383\n",
      "-3.369997944436753\n",
      "-3.3987710381056506\n",
      "-3.4268141898074203\n",
      "-3.453866561161971\n",
      "-3.4801981123584085\n",
      "-3.505801161152243\n",
      "-3.53035847847765\n",
      "-3.553866684538579\n",
      "-3.57660510939431\n",
      "-3.5985924555031357\n",
      "-3.61982553952825\n",
      "-3.6399783631926224\n",
      "-3.659046105009832\n",
      "-3.6773459618907696\n",
      "-3.6947954584634948\n",
      "-3.711144753974111\n",
      "-3.7263890169760656\n",
      "-3.740706415373482\n",
      "-3.7539124976843614\n",
      "-3.7661887536451086\n",
      "-3.7773497578880515\n",
      "-3.7874022614137957\n",
      "-3.796378617406201\n",
      "-3.8045574061858485\n",
      "-3.8116186018881804\n",
      "-3.817603173007416\n",
      "-3.8227832537197663\n",
      "-3.8271528969838644\n",
      "-3.830706071084692\n",
      "-3.8333672748096994\n",
      "-3.8352008607148997\n",
      "-3.836197318907082\n",
      "-3.8363406735385523\n",
      "-3.8353314368799816\n",
      "time consumed 3.3418495655059814\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state = env.reset()\n",
    "print(state)\n",
    "\n",
    "counter = 0\n",
    "time_start = time.time()\n",
    "\n",
    "#while True:\n",
    "for i in range(100):\n",
    "  env.render(mode=\"human\")\n",
    "\n",
    "  action = env.action_space.sample()\n",
    "\n",
    "  state, reward, done, _ = env.step(action)\n",
    "\n",
    "  print(reward)\n",
    "\n",
    "  # For very fast running environments ...\n",
    "  time.sleep(0.02)\n",
    "\n",
    "  if done:\n",
    "    print(\"Resetting\")\n",
    "    env.reset()\n",
    "  #end\n",
    "#end\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"time consumed\", time.time() - time_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Init qtable and policy table\n",
    "\n",
    "- main idea: discretize simple performing a rounding operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  (3,)\n",
      "Action bounds:  [0. 0. 0.] [50.  5. 50.]\n",
      "Action dim:  [51.  6. 51.]\n",
      "action sample:  [28.  2. 28.] [28.161287   2.0833988 28.354046 ] \n",
      " \n",
      "\n",
      "State size  (3,)\n",
      "State bounds:  [0. 0. 0.] [200. 200.  40.]\n",
      "State dim:  [201. 201.  41.]\n",
      "state sample:  [154.  24.   3.] [153.82169   23.722378   2.995409]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "action_size = env.action_space.shape\n",
    "print(\"Action size \", action_size)\n",
    "print(\"Action bounds: \", env.action_space.low, env.action_space.high)\n",
    "action_dim = env.action_space.high -env.action_space.low +1\n",
    "print(\"Action dim: \", action_dim )\n",
    "\n",
    "a_sample = env.action_space.sample()\n",
    "print(\"action sample: \", np.round(a_sample), a_sample, \"\\n \\n\")\n",
    "\n",
    "\n",
    "state_size = env.observation_space.shape\n",
    "print(\"State size \", state_size)\n",
    "print(\"State bounds: \", env.observation_space.low, env.observation_space.high)\n",
    "state_dim = env.observation_space.high -env.observation_space.low +1\n",
    "print(\"State dim: \", state_dim )\n",
    "\n",
    "sample = env.observation_space.sample()\n",
    "print(\"state sample: \", np.round(sample), sample)\n",
    "state_dim_real = np.array([20, 20 , 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[20 20  2 51  6 51]\n",
      "12484800\n"
     ]
    }
   ],
   "source": [
    "#Initializing the Q-matrix \n",
    "print(type(state_dim))\n",
    "dim = np.concatenate((state_dim_real, action_dim)).astype(int)\n",
    "print(dim)\n",
    "Q = np.zeros(dim)\n",
    "print(Q.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "policy = np.zeros(state_dim_real, dtype=type(a_sample))\n",
    "action = np.argmax(Q[5, 5, 1])\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init agent parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.7\n",
    "total_episodes = 10000000\n",
    "alpha = 0.001\n",
    "gamma = 0.93\n",
    "number_of_actions = action_dim[0] * action_dim[1] * action_dim[2]\n",
    "render_every = 5000\n",
    "max_steps = 10000\n",
    "dec_eps = True\n",
    "\n",
    "if dec_eps:\n",
    "    # Exploration parameters\n",
    "    epsilon = 1.0                 # Exploration rate\n",
    "    epsilon_max = 1.0             # Exploration probability at start\n",
    "    epsilon_min = 0.01            # Minimum exploration probability \n",
    "    epsilon_decay_rate = 0.00001             # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, number_of_actions):\n",
    "    if np.random.rand() > epsilon:\n",
    "        action = policy[tuple(state)]\n",
    "        return action\n",
    "    else:\n",
    "        action = np.random.randint(0, number_of_actions)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epsilon(episode):\n",
    "        return epsilon_min + (epsilon_max - epsilon_min) * np.exp(-epsilon_decay_rate * episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sarsa():\n",
    "\n",
    "    # Start the training\n",
    "    for i in range(total_episodes):\n",
    "        epsilon = get_epsilon(i)\n",
    "        state = env.reset() # reseting the environment\n",
    "        state_idx = np.round(state)\n",
    "        # Set a flag for when environment to be rendered\n",
    "        if i % render_every is 0:\n",
    "            print(i)\n",
    "            do_rendering = True\n",
    "        else:\n",
    "            do_rendering = False\n",
    "        episode_done = False\n",
    "        while not episode_done:\n",
    "            # Getting an action\n",
    "            action = epsilon_greedy(state, number_of_actions)\n",
    "            action_idx = np.round(action)\n",
    "\n",
    "            # Taking the action in the environment\n",
    "            next_states, reward, episode_done, _ = env.step(action)\n",
    "            new_state_idx = np.round(new_state)\n",
    "            \n",
    "            old_idx = np.concatenate((state_idx, action_idx))\n",
    "            new_action = np.round(policy(new_state_idx))\n",
    "            new_idx = np.concatenate((new_state_idx, new_action))\n",
    "            \n",
    "\n",
    "            qtable[old_idx] += alpha * (\n",
    "                    reward + (gamma * qtable[new_idx] \n",
    "                              - qtable[old_idx]))\n",
    "            # Update policy\n",
    "            policy[tuple(state)] = action\n",
    "            temp_max_Q = np.abs(qtable[old_idx])\n",
    "            \n",
    "            for pid_p in range(action_dim[0]):\n",
    "                for pid_i in range(action_dim[1]):\n",
    "                    for pid_d in range(action_dim[3]):\n",
    "                        a_pol_idx = np.array([pid_p, pid_i, pid_d])\n",
    "                        pol_idx = np.concatenate((state_idx, a_pol_idx))\n",
    "                        if qtable[pol_idx] > temp_max_Q:\n",
    "                            policy[state_idx] = action\n",
    "                            temp_max_Q = qtable[pol_idx]\n",
    "\n",
    "            # print(\"New best policy for state: \",self.discrete_states, self.policy[self.discrete_states], episode_done, self.take_off_procedure - current_step)\n",
    "            if do_rendering:\n",
    "                env.render()\n",
    "\n",
    "            if episode_done:\n",
    "                env.close()\n",
    "                break\n",
    "\n",
    "            states = next_states\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        #epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def qlearn():\n",
    "    # 2 For life or until learning is stopped\n",
    "    for episode in range(total_episodes):\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state_idx = tuple(state)\n",
    "        step = 0\n",
    "        done = False\n",
    "        \n",
    "        if episode % render_every is 0:\n",
    "            print(episode)\n",
    "            do_rendering = True\n",
    "        else:\n",
    "            do_rendering = False\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # 3. Choose an action a in the current world state (s)\n",
    "            # Getting an action\n",
    "            action = epsilon_greedy(state, number_of_actions)\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_state_idx = tuple(new_state)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            qtable[action][state_idx] = qtable[action][state_idx] + alpha * (reward + gamma * \n",
    "                                        np.max([tab[new_state_idx] for tab in qtable.values()]) - qtable[action][state_idx])\n",
    "\n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            state_idx = tuple(state)\n",
    "            \n",
    "            \n",
    "            if do_rendering:\n",
    "                env.render()\n",
    "            # If done : finish episode\n",
    "            if done == True:\n",
    "                env.close()\n",
    "                break\n",
    "\n",
    "        episode += 1\n",
    "\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sarsa()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from time import perf_counter\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([(5, 5, 15), (0, 0, 0), (0, 0, 0), (0, 0, 0)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().values() # to be added in review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.396454 ,  1.5858614, 30.788624 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 100000\n",
    "# nA = env.action_space.n # to be added in review\n",
    "nA = 3\n",
    "Q = defaultdict(lambda: np.zeros(nA, dtype=float))\n",
    "discount_factor = 0.9  \n",
    "epsilon = 0.1\n",
    "env_max_steps = 1000\n",
    "render = False\n",
    "test_episodes = 1000\n",
    "train_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy():\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    Returns:\n",
    "        A function that takes the observation (state) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy_fn(observation):\n",
    "        prob = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        optimal_action = np.argmax(Q[tuple(observation)])\n",
    "        prob[optimal_action] += (1.0 - epsilon)\n",
    "        return prob\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_train():\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    policy_fn = make_epsilon_greedy_policy()\n",
    "    counter = perf_counter()\n",
    "\n",
    "    for i_episode in range(1, train_episodes + 1):\n",
    "        episode = []\n",
    "        state = env.reset().values()\n",
    "        \n",
    "        if i_episode % 5000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, train_episodes), end=\"\")\n",
    "#             print('no of states explored:', len(Q.keys()))\n",
    "            print('Time taken:', (perf_counter() - counter) / 60, 'minutes')\n",
    "            counter = perf_counter()\n",
    "#             render = True\n",
    "            sys.stdout.flush()\n",
    "        else:\n",
    "            render = False\n",
    "\n",
    "        reward_episode = 0.0\n",
    "        for step in range(env_max_steps):\n",
    "            if state in Q:\n",
    "                prob_values = policy_fn(state)\n",
    "                action = np.random.choice(np.arange(nA), p=prob_values)\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((next_state, action, reward))\n",
    "            reward_episode += reward\n",
    "\n",
    "            if render:\n",
    "                env.render(mode='rgb_array')\n",
    "\n",
    "            if done:\n",
    "                env.close()\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "\n",
    "        train_rewards.append(reward_episode)\n",
    "        sa_in_episode = set([(x[0], x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Find the first occurrence of the (state, action) pair in the episode\n",
    "            first_occurrence_idx = next(i for i, x in enumerate(episode)\n",
    "                                        if x[0] == state and x[1] == action)\n",
    "            # Sum up all rewards since the first occurrence\n",
    "            G = sum([x[2] * (discount_factor ** i) for i, x in enumerate(episode[first_occurrence_idx:])])\n",
    "            # Calculate average return for this state over all sampled episodes\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "\n",
    "    return train_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-21f4966e3a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_control_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-2e7322219069>\u001b[0m in \u001b[0;36mmc_control_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mreward_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mprob_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "rewards = mc_control_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
